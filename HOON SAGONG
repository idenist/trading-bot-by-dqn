import gymnasium as gym
from gymnasium import spaces
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import random
from collections import deque, namedtuple
import matplotlib.pyplot as plt

#############################################
# 0. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ í•¨ìˆ˜
#############################################
def load_trade_data(csv_path, chunksize=5_000_000):
    print(f"ğŸ“‚ CSV ê²½ë¡œ: {csv_path}")
    print("ğŸ“¥ CSVë¥¼ chunk ë‹¨ìœ„ë¡œ ì½ëŠ” ì¤‘...")
    chunk_list = []
    for i, chunk in enumerate(pd.read_csv(csv_path, chunksize=chunksize, low_memory=False)):
        print(f"ğŸ“¦ Chunk {i+1} ë¡œë“œë¨ (í–‰ ìˆ˜: {len(chunk)})")
        # TR_CORRê°€ 0ì¸ í–‰ë§Œ ì„ íƒ
        chunk = chunk[chunk['TR_CORR'] == 0].copy()
        # ë‚ ì§œ ë° ì‹œê°„ ì „ì²˜ë¦¬
        chunk['DATE'] = pd.to_datetime(chunk['DATE'])
        chunk['time_delta'] = pd.to_timedelta(chunk['TIME_M'])
        chunk['full_datetime'] = chunk['DATE'] + chunk['time_delta']
        # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
        chunk = chunk[['full_datetime', 'PRICE', 'SIZE', 'SYM_ROOT']]
        chunk_list.append(chunk)
    df = pd.concat(chunk_list).reset_index(drop=True)
    print(f"âœ… ì „ì²´ ë³‘í•© ì™„ë£Œ, ì´ í–‰ ìˆ˜: {len(df)}")
    return df

def prepare_tick_data(df, symbol='AAPL', time_window='1T'):
    """
    í‹± ë°ì´í„°ë¥¼ ì‹œê°„ì°½ ë‹¨ìœ„ë¡œ OHLCV í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    time_window: ì˜ˆ) '1T': 1ë¶„, '5T': 5ë¶„, '1S': 1ì´ˆ
    """
    # íŠ¹ì • ì¢…ëª©ë§Œ í•„í„°ë§
    symbol_df = df[df['SYM_ROOT'] == symbol].copy()
    
    # ì‹œê°„ ìˆœì„œë¡œ ì •ë ¬
    symbol_df = symbol_df.sort_values('full_datetime')
    
    # ì‹œê°„ì°½ë³„ë¡œ OHLCV ê³„ì‚°
    ohlcv = symbol_df.set_index('full_datetime').resample(time_window).agg({
        'PRICE': ['first', 'max', 'min', 'last'],
        'SIZE': 'sum'
    })
    
    # ë©€í‹°ë ˆë²¨ ì¹¼ëŸ¼ì„ ë‹¨ì¼ ë ˆë²¨ë¡œ ë³€ê²½
    ohlcv.columns = ['Open', 'High', 'Low', 'Close', 'Volume']
    
    # ì¸ë±ìŠ¤ë¥¼ ë¦¬ì…‹í•˜ê³  ë‚ ì§œ/ì‹œê°„ ì¹¼ëŸ¼ ì¶”ê°€
    ohlcv = ohlcv.reset_index()
    
    # NaNì´ ìˆëŠ” í–‰ ì œê±° (ê±°ë˜ê°€ ì—†ëŠ” ì‹œê°„ì°½)
    ohlcv = ohlcv.dropna()
    
    return ohlcv

def split_data_for_training(df, train_ratio=0.7):
    """
    ì‹œê°„ ìˆœì„œì— ë”°ë¼ ë°ì´í„°ë¥¼ í•™ìŠµìš©ê³¼ í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ë¶„ë¦¬
    """
    total_records = len(df)
    train_size = int(total_records * train_ratio)
    
    train_data = df.iloc[:train_size].copy()
    test_data = df.iloc[train_size:].copy()
    
    print(f"í•™ìŠµ ë°ì´í„°: {len(train_data)} ë ˆì½”ë“œ ({train_data['full_datetime'].min()} ~ {train_data['full_datetime'].max()})")
    print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_data)} ë ˆì½”ë“œ ({test_data['full_datetime'].min()} ~ {test_data['full_datetime'].max()})")
    
    return train_data, test_data

#############################################
# 1. í™˜ê²½: StockTradingEnv (ê±°ë˜ ë¹„ìš© ë° ìˆ˜ëŸ‰ ì œí•œ ì¶”ê°€)
#############################################
class StockTradingEnv(gym.Env):
    metadata = {'render.modes': ['human']}

    def __init__(self, df):
        super(StockTradingEnv, self).__init__()
        self.df = df
        self.max_steps = len(df) - 2  # ìµì¼ ê°€ê²© ì‚¬ìš©ì„ ìœ„í•´ -2
        self.current_step = 0
        self.initial_balance = 10000    # ì´ˆê¸° ìë³¸ $10,000
        self.balance = self.initial_balance
        self.shares_held = 0
        self.avg_buy_price = 0.0
        # ê±°ë˜ ë¹„ìš©: 0.1%ì˜ ê±°ë˜ ë¹„ìš© ì ìš©
        self.transaction_cost = 0.001

        # ìƒíƒœ ê³µê°„: OHLCV 5ê°œ í”¼ì²˜
        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(5,), dtype=np.float32)
        # í–‰ë™ ê³µê°„: 2 (0: ê±°ë˜, 1: í™€ë“œ)
        self.action_space = spaces.Discrete(2)

    def _next_observation(self):
        # OHLCV ë°ì´í„°ë¥¼ ìƒíƒœë¡œ ì‚¬ìš©
        obs = np.array([
            self.df['Open'].iloc[self.current_step],
            self.df['High'].iloc[self.current_step],
            self.df['Low'].iloc[self.current_step],
            self.df['Close'].iloc[self.current_step],
            self.df['Volume'].iloc[self.current_step]
        ], dtype=np.float32)
        return obs

    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        self.balance = self.initial_balance
        self.shares_held = 0
        self.avg_buy_price = 0.0
        self.current_step = 0
        return self._next_observation(), {}

    # quantity: ê±°ë˜ ì‹œë„í•  ì£¼ì‹ ìˆ˜ (ê¸°ë³¸ê°’ 1)
    def step(self, action, quantity=1):
        done = False
        self.current_step += 1

        current_price = self.df['Close'].iloc[self.current_step]
        if self.current_step + 1 < len(self.df):
            next_price = self.df['Close'].iloc[self.current_step + 1]
        else:
            next_price = current_price

        reward = 0.0

        # Buy Regime: ì”ê³ ê°€ ì¶©ë¶„í•˜ë©´ ë§¤ìˆ˜ ëª¨ë“œ
        if self.balance >= current_price:
            if action == 0:  # Buy action
                max_shares = int(self.balance // current_price)
                quantity = min(quantity, max_shares)
                # ê±°ë˜ ë¹„ìš©
                cost = self.transaction_cost * quantity * current_price
                # ì—…ë°ì´íŠ¸: ë§¤ìˆ˜ ì‹œì—ëŠ” í‰ê·  ë§¤ìˆ˜ê°€ ê°±ì‹  (ê°€ì¤‘í‰ê· )
                if self.shares_held == 0:
                    self.avg_buy_price = current_price
                else:
                    self.avg_buy_price = ((self.avg_buy_price * self.shares_held) + (current_price * quantity)) / (self.shares_held + quantity)
                self.shares_held += quantity
                self.balance -= quantity * current_price + cost
                # ë³´ìƒ: (ìµì¼ ê°€ê²© ìƒìŠ¹ë¥ )ì—ì„œ ê±°ë˜ ë¹„ìš© ë°˜ì˜
                reward = (next_price - current_price) / current_price - self.transaction_cost
            elif action == 1:  # Hold in Buy Regime
                reward = - (next_price - current_price) / current_price

        # Sell Regime: ì”ê³  ë¶€ì¡±í•˜ê±°ë‚˜ ì£¼ì‹ì„ ë³´ìœ  ì¤‘ì´ë©´ íŒë§¤ ëª¨ë“œ
        elif self.shares_held > 0:
            if action == 0:  # Sell action
                quantity = min(quantity, self.shares_held)
                cost = self.transaction_cost * quantity * current_price
                profit_ratio = (current_price - self.avg_buy_price) / (self.avg_buy_price + 1e-6)
                reward = profit_ratio - self.transaction_cost
                self.balance += quantity * current_price - cost
                self.shares_held -= quantity
                if self.shares_held == 0:
                    self.avg_buy_price = 0.0
            elif action == 1:  # Hold in Sell Regime
                reward = - (current_price - next_price) / current_price
        else:
            reward = 0.0

        if self.current_step >= self.max_steps:
            done = True

        obs = self._next_observation()
        return obs, reward, done, False, {}

    def render(self, mode='human', close=False):
        current_price = self.df['Close'].iloc[self.current_step]
        net_worth = self.balance + self.shares_held * current_price
        profit = net_worth - self.initial_balance
        timestamp = self.df['full_datetime'].iloc[self.current_step].strftime('%Y-%m-%d %H:%M:%S.%f') if 'full_datetime' in self.df.columns else f"Step {self.current_step}"
        print(f"Time: {timestamp}, Balance: {self.balance:.2f}, Shares: {self.shares_held}, Price: {current_price:.2f}, Profit: {profit:.2f}")

#############################################
# 2. Agent, DQN, ReplayBuffer (ê¸°ì¡´ ì½”ë“œ ìœ ì§€)
#############################################
Transition = namedtuple('Transition', ('state', 'action', 'reward', 'next_state', 'done'))

class ReplayBuffer:
    def __init__(self, capacity=1000):
        self.memory = deque(maxlen=capacity)
    def add(self, *args):
        self.memory.append(Transition(*args))
    def sample(self, batch_size):
        return random.sample(self.memory, batch_size)
    def __len__(self):
        return len(self.memory)

class DQN(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(input_dim, 128)
        self.fc2 = nn.Linear(128, 128)
        self.fc3 = nn.Linear(128, 128)
        self.fc4 = nn.Linear(128, output_dim)
    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = F.relu(self.fc3(x))
        return self.fc4(x)

class Agent:
    def __init__(self, state_size, action_size, lr=0.001, gamma=0.001, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995):
        self.state_size = state_size
        self.action_size = action_size
        self.gamma = gamma

        self.epsilon = epsilon
        self.epsilon_min = epsilon_min
        self.epsilon_decay = epsilon_decay

        self.policy_net = DQN(state_size, action_size)
        self.target_net = DQN(state_size, action_size)
        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)

        self.buffer = ReplayBuffer()
        self.update_target_net()

    def update_target_net(self):
        self.target_net.load_state_dict(self.policy_net.state_dict())

    def act(self, state):
        if np.random.rand() < self.epsilon:
            return np.random.choice(self.action_size)
        state = torch.FloatTensor(state).unsqueeze(0)
        with torch.no_grad():
            return self.policy_net(state).argmax().item()

    def remember(self, *args):
        self.buffer.add(*args)

    def replay(self, batch_size):
        if len(self.buffer) < batch_size:
            return
        transitions = self.buffer.sample(batch_size)
        batch = Transition(*zip(*transitions))

        state_batch = torch.FloatTensor(batch.state)
        action_batch = torch.LongTensor(batch.action).unsqueeze(1)
        reward_batch = torch.FloatTensor(batch.reward).unsqueeze(1)
        next_state_batch = torch.FloatTensor(batch.next_state)
        done_batch = torch.FloatTensor(batch.done).unsqueeze(1)

        current_q = self.policy_net(state_batch).gather(1, action_batch)
        next_q = self.target_net(next_state_batch).max(dim=1)[0].detach().unsqueeze(1)
        expected_q = reward_batch + self.gamma * next_q * (1 - done_batch)

        loss = F.mse_loss(current_q, expected_q)

        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay

#############################################
# 3. ì—ì´ì „íŠ¸ í›ˆë ¨ í•¨ìˆ˜
#############################################
def train(env, buy_agent, sell_agent, num_episodes=20, batch_size=32, target_update_freq=4):
    episode_rewards = []
    
    for episode in range(num_episodes):
        state, _ = env.reset()
        done = False
        total_reward = 0
        step = 0

        while not done:
            current_price = env.df['Close'].iloc[env.current_step]

            # regime ê²°ì •: ì”ê³ ê°€ ì¶©ë¶„í•˜ë©´ buy_agent, ë³´ìœ  ì£¼ì‹ì´ ìˆìœ¼ë©´ sell_agent ì„ íƒ
            if env.balance >= current_price:
                agent = buy_agent
                max_quantity = int(env.balance // current_price)
                quantity = random.randint(1, max_quantity) if max_quantity > 0 else 1
            elif env.shares_held > 0:
                agent = sell_agent
                max_quantity = env.shares_held
                quantity = random.randint(1, max_quantity) if max_quantity > 0 else 1
            else:
                next_state, reward, done, _, _ = env.step(1, quantity=1)
                state = next_state
                total_reward += reward
                continue

            action = agent.act(state)
            next_state, reward, done, _, _ = env.step(action, quantity=quantity)

            agent.remember(state, action, reward, next_state, done)
            agent.replay(batch_size)

            state = next_state
            total_reward += reward
            step += 1

        if (episode + 1) % target_update_freq == 0:
            buy_agent.update_target_net()
            sell_agent.update_target_net()

        episode_rewards.append(total_reward)
        print(f"Episode {episode+1}/{num_episodes} - Total reward: {total_reward:.2f} - Steps: {step}")
    
    return episode_rewards

def test(env, buy_agent, sell_agent):
    """
    í•™ìŠµëœ ì—ì´ì „íŠ¸ë¡œ í…ŒìŠ¤íŠ¸í•˜ëŠ” í•¨ìˆ˜
    """
    state, _ = env.reset()
    done = False
    total_reward = 0
    trades = []
    
    print("\n===== í…ŒìŠ¤íŠ¸ ì‹œì‘ =====")
    
    while not done:
        current_price = env.df['Close'].iloc[env.current_step]
        timestamp = env.df['full_datetime'].iloc[env.current_step] if 'full_datetime' in env.df.columns else f"Step {env.current_step}"
        
        # regime ê²°ì •
        if env.balance >= current_price:
            agent = buy_agent
            max_quantity = int(env.balance // current_price)
            quantity = max_quantity if max_quantity > 0 else 1
            agent_type = "Buy"
        elif env.shares_held > 0:
            agent = sell_agent
            quantity = env.shares_held
            agent_type = "Sell"
        else:
            action = 1  # í™€ë“œ
            next_state, reward, done, _, _ = env.step(action, quantity=1)
            state = next_state
            total_reward += reward
            continue
        
        action = agent.act(state)
        next_state, reward, done, _, _ = env.step(action, quantity=quantity)
        
        # í˜„ì¬ ìƒíƒœ ì¶œë ¥
        action_name = "ë§¤ìˆ˜" if action == 0 and agent_type == "Buy" else \
                      "ë§¤ë„" if action == 0 and agent_type == "Sell" else "í™€ë“œ"
        
        # ê±°ë˜ ê¸°ë¡
        trades.append({
            'timestamp': timestamp,
            'price': current_price,
            'action': action_name,
            'quantity': quantity if action == 0 else 0,
            'balance': env.balance,
            'shares': env.shares_held,
            'reward': reward
        })
        
        env.render()
        
        state = next_state
        total_reward += reward
    
    final_balance = env.balance + env.shares_held * env.df['Close'].iloc[-1]
    profit_pct = (final_balance - env.initial_balance) / env.initial_balance * 100
    
    print("\n===== í…ŒìŠ¤íŠ¸ ê²°ê³¼ =====")
    print(f"ìµœì¢… ìì‚°: ${final_balance:.2f}")
    print(f"ìˆ˜ìµë¥ : {profit_pct:.2f}%")
    print(f"ì´ ë³´ìƒ: {total_reward:.2f}")
    
    # ê±°ë˜ ê¸°ë¡ì„ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ë³€í™˜
    trades_df = pd.DataFrame(trades)
    return trades_df

def plot_test_results(trades_df, test_data):
    """
    í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ ì‹œê°í™”í•˜ëŠ” í•¨ìˆ˜
    """
    # íƒ€ì„ìŠ¤íƒ¬í”„ì™€ ìˆœìì‚° ì¶”ì¶œ
    timestamps = test_data['full_datetime'].values
    prices = test_data['Close'].values
    
    # ìˆœìì‚° ê³„ì‚°
    net_worths = trades_df['balance'] + trades_df['shares'] * trades_df['price']
    
    plt.figure(figsize=(14, 10))
    
    # ì£¼ê°€ ê·¸ë˜í”„
    plt.subplot(2, 1, 1)
    plt.plot(timestamps, prices, label='Stock Price')
    plt.title('Stock Price During Test Period')
    plt.xlabel('Time')
    plt.ylabel('Price ($)')
    plt.grid(True)
    plt.legend()
    
    # ìˆœìì‚° ê·¸ë˜í”„
    plt.subplot(2, 1, 2)
    plt.plot(trades_df['timestamp'], net_worths, label='Portfolio Value')
    
    # ë§¤ìˆ˜/ë§¤ë„ í‘œì‹œ
    buys = trades_df[trades_df['action'] == 'ë§¤ìˆ˜']
    sells = trades_df[trades_df['action'] == 'ë§¤ë„']
    
    plt.scatter(buys['timestamp'], buys['price'], marker='^', color='g', s=100, label='Buy')
    plt.scatter(sells['timestamp'], sells['price'], marker='v', color='r', s=100, label='Sell')
    
    plt.title('Portfolio Value and Trading Actions')
    plt.xlabel('Time')
    plt.ylabel('Value ($)')
    plt.grid(True)
    plt.legend()
    
    plt.tight_layout()
    plt.savefig('trading_results.png')
    plt.show()

#############################################
# 4. ë©”ì¸ í•¨ìˆ˜
#############################################
def main():
    # ë°ì´í„° ë¡œë“œ
    csv_path = r"D:\wrds\taq_aapl_nvda_2018\aapl_nvda_2018.csv"
    df = load_trade_data(csv_path)
    
    # ë°ì´í„° í•„í„°ë§ (ì˜ˆ: AAPL ì¢…ëª©ë§Œ ì„ íƒ)
    symbol = 'AAPL'
    
    # í‹± ë°ì´í„°ë¥¼ ì‹œê°„ ê°„ê²©ìœ¼ë¡œ ë¦¬ìƒ˜í”Œë§ (ì˜ˆ: 1ë¶„ ê°„ê²©)
    tick_data = prepare_tick_data(df, symbol=symbol, time_window='1T')
    
    # í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ë¦¬
    train_data, test_data = split_data_for_training(tick_data, train_ratio=0.7)
    
    # í™˜ê²½ ìƒì„±
    train_env = StockTradingEnv(train_data)
    
    # ìƒíƒœ ë° í–‰ë™ í¬ê¸° ì„¤ì •
    state_size = 5  # OHLCV ë°ì´í„° (5ê°œ í”¼ì²˜)
    action_size = 2  # 0: ê±°ë˜, 1: í™€ë“œ
    
    # ì—ì´ì „íŠ¸ ìƒì„±
    buy_agent = Agent(state_size, action_size)
    sell_agent = Agent(state_size, action_size)
    
    # í•™ìŠµ ì§„í–‰
    print("\n===== í•™ìŠµ ì‹œì‘ =====")
    episode_rewards = train(train_env, buy_agent, sell_agent, num_episodes=20, batch_size=32, target_update_freq=4)
    
    # í…ŒìŠ¤íŠ¸ í™˜ê²½ ìƒì„±
    test_env = StockTradingEnv(test_data)
    
    # í…ŒìŠ¤íŠ¸ ì§„í–‰
    trades_df = test(test_env, buy_agent, sell_agent)
    
    # ê²°ê³¼ ì‹œê°í™”
    plot_test_results(trades_df, test_data)
    
    return buy_agent, sell_agent, trades_df

if __name__ == "__main__":
    main()
